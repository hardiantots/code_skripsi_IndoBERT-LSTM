{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_remove(text):\n",
    "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+|#\\S+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
    "    text = re.sub(r\"'Simak.*\", '', text)\n",
    "    text = re.sub(r'\"Simak.*', '', text)\n",
    "    text = re.sub(r\"'Baca halaman.*\", '', text)\n",
    "    text = re.sub(r'\\[Gambas:.*', '', text)\n",
    "    text = re.sub(r'Lihat juga.*', '', text)\n",
    "    text = re.sub(r'Lihat video.*', '', text)\n",
    "    text = re.sub(r'Lihat Video.*', '', text)\n",
    "    text = re.sub(r\"'Artikel.*\", '', text)\n",
    "    text = re.sub(r'\"Artikel.*', '', text)\n",
    "    text = re.sub(r\"'NEXT:.*\", '', text)\n",
    "    text = re.sub(r'\"NEXT:.*', '', text)\n",
    "    text = re.sub(r\" {4}Halaman.*\", '', text)\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+|#\\S+', ' ', text)\n",
    "    text = re.sub('NARASI:', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r':\\w+?:', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]|_', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    # Menghapus stopword\n",
    "    stop_words_nltk = stopwords.words('indonesian')\n",
    "    filter_words = [word for word in words if word not in stop_words_nltk]\n",
    "    new_text = ' '.join(filter_words)\n",
    "    new_text = re.sub(r'\\s+', ' ', new_text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "bert_tokenizer.save_vocabulary('.')\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return {\n",
    "        'input_ids': torch.tensor([encoded.ids]),  # Tambahkan batch dimension\n",
    "        'attention_mask': torch.tensor([encoded.attention_mask])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained IndoBERT model\n",
    "model_name = 'indobenchmark/indobert-base-p1'\n",
    "indobert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "for param in indobert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[768, 128, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "# Definisikan model LSTM\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_size, fc_hidden_sizes, output_size, dropout_prob=0.6):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "        # Fully connected layers\n",
    "        layer_sizes = [lstm_hidden_size] + fc_hidden_sizes\n",
    "        print(layer_sizes)\n",
    "        fc_layers = []\n",
    "\n",
    "        fc_layers.append(nn.Linear(layer_sizes[0], layer_sizes[1]))\n",
    "        fc_layers.append(nn.BatchNorm1d(layer_sizes[1])) \n",
    "        fc_layers.append(nn.Dropout(dropout_prob))  \n",
    "        fc_layers.append(nn.ReLU())\n",
    "\n",
    "        for i in range(1, len(fc_hidden_sizes)):\n",
    "            fc_layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            fc_layers.append(nn.ReLU())\n",
    "\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(fc_hidden_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        lstm_out, _ = self.lstm(input_ids)\n",
    "\n",
    "        # Only take the output from the last timestep\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        # print(f\"Shape dari lstm_out setelah LSTM: {lstm_out.shape}\")\n",
    "\n",
    "        # Feedforward through fully connected layers\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        # print(f\"Shape dari fc_out setelah fully connected layers: {fc_out.shape}\")\n",
    "\n",
    "        # Feedforward through output layer\n",
    "        logits = self.output_layer(fc_out)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Dimensi input untuk LSTM\n",
    "input_size = indobert_model.config.hidden_size\n",
    "hidden_size = 768\n",
    "fc_hidden_sizes = [128, 32, 32]\n",
    "output_size = 2  # Jumlah kelas (hoax dan non-hoax)\n",
    "\n",
    "# Inisialisasi model LSTM\n",
    "lstm_model = LSTMClassifier(input_size, hidden_size, fc_hidden_sizes, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisikan model gabungan\n",
    "class IndoBERT_LSTM(nn.Module):\n",
    "    def __init__(self, indobert_model, lstm_model):\n",
    "        super(IndoBERT_LSTM, self).__init__()\n",
    "        self.indobert = indobert_model\n",
    "        self.lstm = lstm_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through IndoBERT\n",
    "        with torch.no_grad():\n",
    "            outputs = self.indobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = outputs[0]\n",
    "\n",
    "        # # Debugging shape of last_hidden_state\n",
    "        # print(f\"last_hidden_state shape: {last_hidden_state.shape}\")  # Should be [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # Forward pass through LSTMClassifier\n",
    "        logits = self.lstm(last_hidden_state)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Inisialisasi model gabungan\n",
    "model = IndoBERT_LSTM(indobert_model, lstm_model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hoax(input_text, model_source, model_name):\n",
    "    # Pembersihan teks\n",
    "    cleaned_text = noise_remove(input_text)\n",
    "\n",
    "    # Tokenisasi\n",
    "    tokenized = tokenize_text(cleaned_text)\n",
    "    input_ids = tokenized['input_ids']\n",
    "    attention_mask = tokenized['attention_mask']\n",
    "\n",
    "    # Ubah ke tensor\n",
    "    input_ids_tensor = input_ids.to(device)\n",
    "    attention_mask_tensor = attention_mask.to(device)\n",
    "\n",
    "    # Memuat model dan pindahkan ke perangkat yang sesuai\n",
    "    model = IndoBERT_LSTM(indobert_model, lstm_model)\n",
    "    model.load_state_dict(torch.load(f'{model_source}/{model_name}'))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    input_ids_tensor = input_ids_tensor.to(device)\n",
    "    attention_mask_tensor = attention_mask_tensor.to(device)\n",
    "\n",
    "    # Prediksi\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids_tensor, attention_mask_tensor)\n",
    "\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Interpretasi hasil\n",
    "    if predicted_class == 0:\n",
    "        return \"Prediksi Berita : Non-Hoax\"\n",
    "    else:\n",
    "        return \"Prediksi Berita : Hoax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi Berita : Hoax\n"
     ]
    }
   ],
   "source": [
    "# 1. Input teks manual\n",
    "input_text = input(\"Masukkan teks yang ingin diuji: \")\n",
    "\n",
    "# 2. Prediksi\n",
    "hasil_prediksi = predict_hoax(input_text, \"Dropout Batch 64 p 0.6\", \"indobert_lstm_modelcombine_64_0.6.pth\")\n",
    "\n",
    "# 3. Menampilkan hasil\n",
    "print(hasil_prediksi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
